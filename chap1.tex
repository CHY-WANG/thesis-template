\Chapter{INTRODUCTION}
\label{intro}

Sparse penalized regression models have become increasingly popular in recent years in statistics and machine learning. They carry out variable selection, identifying a small number of features associated with the outcome of interest out of a set that potentially also includes many unimportant features. In doing so, they improve and simplify the scientific interpretation of the model and lend stability to prediction and estimation. They are particularly appealing when dealing with high dimensional data sets that have a large number of features, for example, data sets from gene analysis, image recognition or text mining. 

Sparse penalized regression models cover a broad family of models. First, different regression loss functions can be flexibly chosen according to the type of outcomes we are interested in. For example, we can use linear regression loss (the standard loss) to deal with continuous outcomes, logistic regression loss to deal with binary outcomes, Cox regression loss to deal with censored time outcomes, and so on. Second, we can choose different sparse penalization functions (or penalties) to carry out different variable selection strategies, such as the lasso penalty, elastic net penalty, and group lasso penalty. Due to the limited scope of this thesis, we cannot look at all models in the family, and will instead concentrate on the most representative models. The variety of models we investigate demonstrates the potential of the proposed approaches to be extended to other members in the family, although we do limit our scope to convex models. 

Another advantage of sparse penalized regression models in high dimensional analysis is that very efficient algorithms have been developed to fit these models, with computational burden scaling much better as dimension increases than it does for many alternative approaches. A popular technique that can be combined with any existing algorithm is screening, which removes features known in advance to be sparse from the model optimization problem, thereby reducing it to a low dimensional problem. However, the screening step requires calculation to determine these inactive features; the computation involved potentially causes the screening rule itself to become a new bottleneck in the fitting process. In addition, little work has been done on feature screening outside of the most basic sparse model: lasso-penalized linear regression.

In this thesis, we will both develop a general screening framework that applies to all sparse penalized regression models and derive novel screening methods for two important models: penalized Cox regression and standard elastic net model. We focus on screening methods that reduce the time cost of the whole optimization problem. The screening methods we propose can be safely combined with any other existing optimization algorithms and do not alter the solution obtained and therefore any statistical properties of the solution, as long as the existing optimization algorithms converge to the global optimum. All of our proposed methods are implemented and tested in the publicly available \textbf{biglasso} R package.

In the rest of this chapter, we first introduce some preliminary concepts and results that form the basis of this thesis, and then briefly cover the structure of the rest of the thesis.

\section{Preliminaries}

\subsection{Convex Optimization}

For a function $f:\mathbb{R}^p\xrightarrow[]{}\mathbb{R}$, a vector $\boldsymbol v$ is a subgradient of $f$ at $\boldsymbol x$ iff
\begin{equation}
    f(\boldsymbol x')-f(\boldsymbol x)\geq \boldsymbol v^T(\boldsymbol x'-\boldsymbol x),\quad\forall \boldsymbol x'\in \mathbb{R}^p.
\end{equation}

The subdifferential of $f$ at $\boldsymbol x$, denoted as $\partial f(\boldsymbol x)$, is the set of all subgradients of $f$ at $\boldsymbol x$. When $f$ is convex, and differentiable at $\boldsymbol x$, we simply have $\partial f(\boldsymbol x)=\{\nabla f(\boldsymbol x)\}$.

We consider the following optimization problem:
\begin{gather}
    \begin{aligned}
        \label{eq:primeintro}
        \underset{\boldsymbol x}{\min}\quad &f_0(\boldsymbol x)\\
        \textit{s.t.}\quad &f_i(\boldsymbol x)\leq 0,\quad i=1,2,...,m,\\
        & A\boldsymbol x=\boldsymbol b,
    \end{aligned}
\end{gather}
where $f_0,f_1,...,f_m:\mathbb{R}^p\xrightarrow[]{}\mathbb{R}$, $A\in\mathbb{R}^{n\times p}$ and $\boldsymbol b\in\mathbb{R}^n$. At this point, we have not yet made any assumption about the convexity of the functions $f_0,f_1,...,f_m$. Its (Lagrange) dual problem is the maximization of the following (Lagrange) dual function:
\begin{equation}
    \label{eq:dualintro}
    g(\boldsymbol u,\boldsymbol v)\equiv \underset{\boldsymbol x}{\inf}\left(f_0(\boldsymbol x)+\sum_{i=1}^m\boldsymbol u_if_i(\boldsymbol x)+\boldsymbol v^T(A\boldsymbol x-\boldsymbol b)\right),
\end{equation}
where $\boldsymbol u\in\mathbb{R}^m,\boldsymbol v\in\mathbb{R}^n$, subject to $\boldsymbol u_i\geq 0,\quad i=1,...,m$. The dual function $g$ will always be a concave function, so maximizing it is equivalent to minimizing the convex function $-g$. We call the original problem \eqref{eq:primeintro} the primal problem so the two problems form a pair of primal and dual problems. In this paper, we will also generally call a problem the dual problem of a primal problem if it is the dual problem of a problem equivalent to the primal problem. 

The variables $(\boldsymbol x^*,\boldsymbol u^*,\boldsymbol v^*)$ are said to satisfy the Karush-Kuhn-Tucker (KKT) conditions iff
\begin{gather}
    \label{eq:kktintro}
    \begin{aligned}
        f_i(\boldsymbol x^*)\leq 0&,\quad i=1,...,m\\
        A\boldsymbol x^*=b&,\\
        \boldsymbol u^*_i\geq 0&,\quad i=1,...,m\\
        \boldsymbol u^*_i f_i(\boldsymbol x^*)=0&,\quad i=1,...,m\\
        \boldsymbol 0\in \partial f_0(\boldsymbol x^*)+\sum_{i=1}^m \boldsymbol u_i^*\partial f_i(\boldsymbol x^*)+A^T\boldsymbol v&.
    \end{aligned}
\end{gather}
The KKT conditions are sufficient for $\boldsymbol x^*$ and $(\boldsymbol u^*,\boldsymbol v^*)$ to solve the primal and dual problems respectively. The condition $\boldsymbol u_i^*f_i(\boldsymbol x^*)=0$ is also called complementary slackness, because it requires that for each $i$ at most one of $\boldsymbol u_i^*$ and $f_i(\boldsymbol x^*)$ can be nonzero. Note that for an unconstrained problem, the KKT conditions simply reduce to:
\begin{equation}
    \boldsymbol 0\in \partial f_0(\boldsymbol x^*).
\end{equation}

Stronger connections can be made between primal and dual problems when the functions $f_0,f_1,...,f_m$ are all convex. Without loss of generality, assume the first $k$ constraint functions $f_1,f_2,...,f_k$ are affine and the remaining $m-k$ are not. Then Slater's condition for the primal problem states that there exists an $\boldsymbol x$ such that
\begin{equation}
    f_i(\boldsymbol x)\leq 0,\quad i=1,...,k,\quad f_i(\boldsymbol x)<0,\quad i=k+1,...,m,\quad A\boldsymbol x= \boldsymbol b.
\end{equation}
It is apparent that unconstrained convex problems automatically satisfy Slater's condition. If Slater's condition holds, then the KKT conditions (including the complementary slackness condition) become both sufficient and necessary for a solution. 

For a general optimization problem \eqref{eq:primeintro} and any $(\boldsymbol x^*,\boldsymbol u^*,\boldsymbol v^*)$, the following relationship, known as weak duality, holds:
\begin{equation}
    f_0(\boldsymbol x^*)\geq g(\boldsymbol u^*,\boldsymbol v^*).
\end{equation}
In other words, the duality gap $f_0(\boldsymbol x^*)-g(\boldsymbol u^*,\boldsymbol v^*)$ is non-negative. If the problem further satisfies the Slater's condition, then the \quotes{$\geq$} becomes \quotes{$=$} when $(\boldsymbol x^*,\boldsymbol u^*,\boldsymbol v^*)$ satisfy the KKT conditions, a condition known as strong duality. In other words, the duality gap is $0$. Slater's condition is a sufficient condition for the results above, but not necessary. There are other forms of sufficient conditions but Slater's condition is mostly commonly considered because it is simple and covers a large family of optimization problems. 

\subsection{Spare Penalized Regression}

We consider the class of sparse penalized regression models where the estimate of the coefficients $\boldsymbol\beta\in\mathbb{R}^p$ for the $p$ features are the solution to the following type of problem:
\begin{equation}
    \label{eq:spr}
    \underset{\boldsymbol\beta}{\min}\quad l(X\boldsymbol\beta|\boldsymbol y)+\lambda \sum_{j=1}^pP(\boldsymbol\beta_j),
\end{equation}
where $X\in\mathbb{R}^{n\times p}=[\boldsymbol x_1,...,\boldsymbol x_p]$ is the feature matrix containing $p$ features measured for each of the $n$ samples. The loss function $l(\cdot|\boldsymbol y):\mathbb{R}^n\xrightarrow[]{}\mathbb{R}$ is strongly convex, differentiable, and depends on both the outcome $\boldsymbol y$ and the distribution used to model the outcome. For example, in linear regression, the loss function is $\norm{\boldsymbol y-X\boldsymbol\beta}_2^2$. Logistic regression, Poisson regression and Cox regression also have this type of loss function. The penalty function $P:\mathbb{R}\xrightarrow[]{}\mathbb{R}_+$ is a convex function satisfying $P'(0^+)=-P'(0^-)>0$ and which is differentiable elsewhere. For example, The lasso penalty $\norm{\boldsymbol\beta}_1$ and the elastic net penalty $\alpha\norm{\boldsymbol\beta}_1+\frac{1-\alpha}{2}\norm{\boldsymbol\beta}_2^2$ both satisfy these conditions. Finally, the regularization parameter $\lambda \ge 0$ is a tuning parameter that control the size of penalization relative to the loss.

In practice, we cannot know the best choice of $\lambda$ beforehand, so we usually solve the model along a path of decreasing $\lambda$ values and afterward chose the $\lambda_l$ that leads to be best model according to some criterion. If we denote the solution given a specific choice of $\lambda$ as $\boldsymbol\beta_\lambda$, then the solutions obtained along the path are called a solution path. Solution paths for convex problems are continuous, so if the $\lambda$ values are closely spaced, then the solutions at two consecutive $\lambda$ values will be close as well. This is important in practice because after the first solution is obtained, it can be used as a warm start for any algorithm solving for the next solution. Pathwise optimization is a very popular practice because it allows fine control over variable selection while at the same time the warm start allows the problem to be solved efficiently even if the path contains a large number of $\lambda$ values. 

It directly follows from the previous convex optimization results that the sufficient and necessary condition for $\boldsymbol\beta_{\lambda}$ to be a solution are the KKT conditions:
\begin{align*}
    x_j^T\nabla l(X\boldsymbol\beta_\lambda)+\lambda P'([\boldsymbol \beta_\lambda]_j)&=0 &&\textnormal{if }\boldsymbol \beta_\lambda]_j\neq 0,\\
    |x_j^T\nabla l(X\boldsymbol\beta_\lambda)|&\leq \lambda P'(0^+) &&\textnormal{if }[\boldsymbol \beta_\lambda]_j= 0,
\end{align*}
for all $j=1,...,p$. As a result, some of the coefficients can be exactly zero in the solution and variable selection is automatically done. Because of their sufficiency, we can use the conditions to find a solution or validate a proposed solution. On the other hand, because of their necessity, if we can know $|x_j^T\nabla l(X\boldsymbol\beta_\lambda)|< \lambda P'(0^+)$ for some $j$ before obtaining the solution $\boldsymbol\beta_\lambda$, we can safely conclude that $[\beta_\lambda]_j=0$. If we denote $\mathcal{S}_\lambda$ as the set of $j$'s where we fail to conclude $[\beta_\lambda]_j=0$ ($\mathcal{S}_\lambda^c$ is the set of $j$'s where we safely conclude  $[\beta_\lambda]_j=0$) then the full problem \eqref{eq:spr} with dimension $p$ will be equivalent to the reduced problem with dimension $|\mathcal{S}_\lambda|$:
\begin{equation}
    \underset{\boldsymbol\beta_{\mathcal{S}_\lambda}}{\min}\quad l(X_{\mathcal{S}_\lambda}\boldsymbol\beta_{\mathcal{S}_\lambda})+\lambda \sum_{j\in\mathcal{S}_\lambda}P(\boldsymbol\beta_j),
\end{equation}
where $X_{\mathcal{S}_\lambda}$ is the submatrix of $X$ containing column $j$ iff $j\in\mathcal{S}_\lambda$ and $\boldsymbol\beta_{\mathcal{S}_\lambda}$ is a subvector of $\boldsymbol\beta$ containing element $j$ iff $j\in\mathcal{S}_\lambda$. All the features in $\mathcal{S}_\lambda^c$ are \quotes{eliminated} from the problem. In the special case when we have $\lambda$ satisfying:
\begin{equation}
    \lambda\geq\lambda_{\max}\equiv \underset{j}{\max}\frac{|\boldsymbol x_j^T\nabla l(\boldsymbol0)|}{P'(0^+)},
\end{equation}
we can safely eliminate all features as $\boldsymbol\beta_{\lambda}=\boldsymbol0$ satisfies the KKT conditions. As a result, if we want to obtain a solution path, it suffices to consider a decreasing sequence of $\lambda$ in $(0,\lambda_{\max}]$.

\section{Thesis Structure}

In Chapter \ref{METHOD1}, we develop a novel adaptive hybrid screening framework that can utilize any safe screening rule to efficiently obtain a solution path for a sparse penalized regression problem. It reuses the same previous solution as a reference to carry out screening for multiple future solutions, greatly reducing the cost of performing screening. In this framework, the reference is updated adaptively at the time that best balances the cost and gain of performing the screening calculations. It can utilize any optimization algorithm, although we focus on coordinate descent \citep{friedman2007pathwise}, as well as additional non-safe screening approaches such as strong rule screening \citep{Tibshirani2012} and active set cycling \citep{lee2007efficient}. It is also easily extendable to any sparse penalized regression model provided that a safe sequential screening rule exists for that model. We show applications to 3 specific models: standard lasso, lasso penalized logistic regression and group lasso using their corresponding safe rules \citep{wang2013lasso,wang2014safe}. We have shown that all these applications outperform other competing state-of-the-art screening methods by a large margin under almost all scenarios.

In Chapter \ref{METHOD2}, we will derive a novel safe screening rule for lasso penalized Cox regression model \citep{cox1972regression} that can update itself utilizing previous solution in the solution path. Cox regression is a powerful semi-parametric model that analyse features that affect subjects' survival when some of the observed survival times are censored, but it introduces a non-separable loss function while all previous safe screening rules focus on models with separable loss functions. We derive the safe screening rule by looking at the dual problem of an equivalent augmented model. We use numerical studies to investigate its performance in both the traditional sequential screening framework and newly proposed adaptive screening framework. We use both numerical and theoretical results to explain its potential disadvantages due to the extra dimensions we introduce in the augmented model.

In Chapter \ref{METHOD3}, we will derive a novel safe screening rule for standard elastic net problem where the elastic net penalty term has the form
\begin{equation}
    \lambda P_\alpha(\boldsymbol\beta)=\alpha \lambda \norm{\boldsymbol\beta}_1+\frac{\alpha}{2}\lambda\norm{\boldsymbol\beta}_2^2.
\end{equation}
This form decomposes the tuning problem into tuning an important tuning parameter $\lambda$ that controls the overall size of penalty and a less important tuning parameter $\alpha$ that controls the penalty's similarity to lasso penalty. In practice, it allows us to tune only the overall size $\lambda$ while keeping $\alpha$ fixed and is favored in most computation packages. Mathematically, the penalty function does not have a fixed shape and cannot be trivially reduced to a lasso model. All previous safe screening methods can only deal with forms of elastic net model that can be reduced to lasso model. To tackle the problem, we consider an intermediate dual problem that is a reshaped problem of the original dual problem and breaks a pair of problems into a pair of reshaping problems and a pair of projecting problems. We build the proposed rule into the adaptive screening framework and show through numerical studies that it outperforms the other existing non-safe screen method sequential strong rule \citep{Tibshirani2012}.

In Chapter \ref{summary}, we summarize the contributions in the thesis and discuss future research directions.