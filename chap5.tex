\Chapter{SUMMARY}
\label{summary}

\section{Contributions}

This thesis focuses on feature screening methods for sparse penalized regression models. Sparse penalized regression models are popular for analysing high-dimensional data sets for their nice statistical properties but large data sets with a massive number of features present a challenge to any algorithm. Feature screening methods can eliminate a great proportion of the features and thus a great proportion of the computation burden from the problem, while not altering the solution or its statistical properties. All the feature screening methods proposed in this thesis are implemented and publicly accessible in the \textbf{biglasso} R package.

In Chapter \ref{METHOD1}, we propose a novel adaptive hybrid screening framework for efficient lasso-type optimization over the solution path. The key idea is to judiciously reuse the reference for screening along the path until the point where updating to a new reference is beneficial. We developed three instances of algorithms using this framework, for standard lasso-penalized linear regression, for sparse logistic regression, and for the group lasso. Extensive numeric experiments on both simulated and real data, including a very large data set that cannot fit into memory, demonstrate that our proposed algorithms outperform other state-of-the-art screening methods uniformly, in many cases several times faster that competing approaches. The proposed adaptive hybrid screening framework is flexible and can be extended in several ways. The framework is general and does not require a specific algorithm, strong rule, or safe rule. As faster algorithms to solve the lasso problem, better safe rules, or improved strong rules are developed, these can be directly plugged into the adaptive hybrid framework with a corresponding improvement in computational efficiency. It can also be easily extended to any sparse penalized regression model if a safe rule for that model exists.

In Chapter \ref{METHOD2}, we propose a novel safe screening rule to help solve pathwise Lasso penalized Cox regression efficiently. It utilizes previous solutions on the path to increase power and eliminate a greater proportion of inactive features. The rule is derived by constructing an augmented dual problem and deriving closed form upper bounds that allow safe elimination of features guaranteed to have zero coefficients at the next point in the solution path. Although the semiparametric nature of Cox regression limits the power of safe screening, we identify factors that govern these limitations and identify cases in which the proposed rules have high power and increase speed when incorporated into the adaptive screening framework. These results also demonstrate the possibility of deriving safe sequential screening rules for lasso-type problems with nonstandard likelihood functions that are not separable and offer insights for developing screening methods for other problems in this class. Furthermore, the performance of the proposed rule in the adaptive screening framework is a proof that the adaptive screening framework will always be close to the best among existing methods in terms of efficiency, even if the safe screening rule is ineffective.

In Chapter \ref{METHOD3}, we propose the SPARSE rule for elastic net that utilizes the previous solution as a reference for enhanced screening power. To derive the rule, we consider an intermediate reshaped dual form that connects the reference and the target, and divide the original problem into a reshaping problem and a projecting problem. We show that it can be incorporated into the adaptive screening framework and performs 3-4 times faster than the existing state-of-the-art method SSR in most cases. Our result covers the form of elastic net problem that is not only more useful in practice, but also mathematically more interesting because it cannot simply be reduced to the lasso problem. In fact our proposed safe screening method is the only existing method we know of that works on a penalty function whose shape varies with $\lambda$. A constant shape penalty $\lambda p(\boldsymbol\beta)$, such as lasso penalty or group lasso penalty can be rewritten as $p(\lambda\boldsymbol\beta)$, and all previous safe screening rules apply only to penalties that fall into this category. The elastic net, however, does not share this property.

\section{Future Directions}

Two other similar adaptive algorithms can be considered for efficiency in some scenarios. First, we can build an adaptive fine tuning on top of adaptive screening. In practice when we solve a model at two consecutive values $\lambda_1$ and $\lambda_2$, a large number of features may enter the model between them. The model at some $\lambda_3\in(\lambda_1,\lambda_2)$ may be helpful for distinguishing the importance of these features and it may be solved easily by reusing the same reference as for screening as $\lambda_2$ does. We can develop a pathwise algorithm where the sequence of $\lambda$ values is not necessarily fixed in advance but determined adaptively as the solution path is obtained, according to the gain from fine tuning and the costs for extra computation. This algorithm can take advantage of the fact adaptive screening performs better when $\lambda$ values are closer. Second, it will be useful to build an adaptive screening method that only requires a strong rule not a safe rule, as strong rule are mathematically much easier to derive for a larger class of models. Similar to current adaptive screening where safe screening at multiple $\lambda$ values share the same reference, the adaptive screening with only strong rule can make multiple $\lambda$ values share the same post-convergence checking to reduce the average cost of checking.

Our augmented approach for lasso penalized Cox regression introduces a large number of additional dual variables, which limits the power of the resulting safe rule. It may be possible to pursue alternative dual constructions that avoid this increase in dimensionality. If such constructions are possible, we believe a much stronger safe screening method can be derived.

An important extension of the work developed here would be the development of safe rules for the elastic net in generalized linear models. To our knowledge, strong rules are the only available screening method for these problems, and no safe rules exist. Since our derivation has already tackled the trickiest part -- the varying shape of the elastic net penalty as $\lambda$ changes -- the techniques developed here may allow the development of safe rules and thus adaptive safe screening for elastic net penalized logistic regression and other problems in this class. More challenging extensions can also be considered. For example, it may be possible to use these techniques to develop safe rules for non-convex penalties or re-weighted lasso penalized regression.
