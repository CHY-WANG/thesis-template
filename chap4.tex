\Chapter{MY THIRD METHOD}
\label{METHOD3}

\section{Introduction}

In a linear regression setting with $n$ samples and $p$ features. Let $\boldsymbol y$ denote the vector of $n$ responses and $X=[\boldsymbol x_1,...,\boldsymbol x_p]$ denote the $n\times p$ matrix of features. The naive form of elastic net problem \citep{zou2005regularization} is the optimization of the following problem 

\begin{equation}
    \label{eq:enet0}
    \underset{\beta\in \mathbb{R}^p}{\mathrm{min}}\frac{1}{2}||\boldsymbol y-X\boldsymbol\beta||_2^2+n\lambda_1||\boldsymbol\beta||_1+\frac{n}{2}\lambda_2||\boldsymbol\beta||_2^2,
\end{equation}

where $\lambda_1,\lambda_2> 0$ are two tuning parameters that control the sizes of $L_1$ and $L_2$ penalties and $\boldsymbol\beta\in\mathbb{R}^p$ is the coefficient vectors for the $p$ features. A useful reparametrization of this problem is:

\begin{equation}
    \label{eq:enet}
    \underset{\beta\in \mathbb{R}^p}{\mathrm{min}}\frac{1}{2}||\boldsymbol y-X\boldsymbol\beta||_2^2+n\alpha\lambda||\boldsymbol\beta||_1+\frac{n(1-\alpha)}{2}\lambda||\boldsymbol\beta||_2^2,
\end{equation}

where the tuning parameter $\lambda>0$ controls the overall size of penalty and $\alpha\in(0,1)$ controls the balance between $L_1$ and $L_2$ penalties. This parametrization allows us to tune only the overall size $\lambda$ which is more relevant, while leaving the less important parameter $\alpha$ as fixed, instead of tuning $\lambda_1$ and $\lambda_2$ at the same time. Since this parameterization makes tuning problem simpler, it is usually more preferable in practice, for example, in the \textbf{glmnet} R package. For the rest of the paper, we will only focus on the reparametrized problem \eqref{eq:enet} considering only $\lambda$ as tuning parameter.

The elastic net is a regularization method very similar to lasso problem \citep{tibshirani1996regression} with an extra $L_2$ penalty. The elastic net shares all the nice properties of lasso: automatic sparse solution and continuous shrinkage. This small difference of the elastic net, furthermore, brings some great advantages over the lasso method. First, the objective function in elastic net problem \eqref{eq:enet} is strictly convex so there will be a unique solution, while lasso problem solution may not be unique especially when the penalty tuning parameter is small. Second, in a case with a group of important but highly correlated features, elastic net tends to select all the them, but the lasso tends to select only one of them.

The elastic net and lasso method have gain great success in the high dimension data analysis, where both the sample size $n$ and number of features $p$ are extremely large. Efficient algorithms to solve them are important because the dimensionality introduces great time and memory cost. One of the most promising techniques that can be applied on top of any other algorithms is \quotes{screening}, which takes advantages of the sparsity of the solution: if we know some of the features that have 0 coefficients in the solution before solving the problem, we can \quotes{discard} those features from the problem. Any solving algorithm can be applied to the reduced problem without those features much faster, but the solution will be exactly the same as the solution to the origin problem. The key is discarding more features and not make wrong discard. Moreover, a typical practice to solve penalized regression problems including elastic net and lasso is to solve the problem sequentially along a path of penalty tuning parameter values $\lambda_0,\lambda_1,...,\lambda_L$ and select the best value. In this case, when solving the problem at a $\lambda_l$, solutions at previous values $\lambda_0,...,\lambda_{l-1}$ are already known and can be utilized to conduct a more powerful screening that discards more features. There have been many studies on screening method for lasso problem \citep{ghaoui2010safe,Tibshirani2012,wang2013lasso,Zeng2021,wang2021adaptive} and they have been proven to bring significant speedup on popular lasso algorithms.

Studies on screening methods for elastic net problem are still very limited. First, if we consider the naive form \eqref{eq:enet0} and consider $\lambda_2$ as fixed, then the $L_2$ penalty can be considered as a part of the first loss term by considering a augmented definition of $X$ and $\boldsymbol y$. Then the problem is equivalent to a trivial lasso problem and any screening methods for lasso problem can be used. However, this form is not favorable in practice for the reasons we have discussed about and we only want to focus on the more popular form \eqref{eq:enet}. Second, the screening rule BEDPP for elastic net \citep{Zeng2021} has been proposed, but it cannot utilize previous solutions and will lose power along the path. Third, there are some unsafe screening rules, such as strong rule \citep{Tibshirani2012}, that can be combined with checks afterwards to make sure no feature is mistakenly discarded and produce safe screening. Strong rule can provide a significant speedup in a large class of problems including lasso and elastic net problems, and has been widely used, for example, in \textbf{glmnet} R package. However, recently, the adaptive screening framework \citep{wang2021adaptive} which adaptively utilizes and reuses previous solutions as reference for any existing safe screening rules, can discard a great proportion of features while having low computation cost and has proven to outperform other screening methods, including strong rule screening, by a large margin in time cost for solving the lasso problem. As a result, a safe screening rule for the elastic net that can utilize previous solutions is desired and then an adaptive screening method can utilize this rule to form an efficient algorithm to solve elastic net problem as well.

In this paper, we propose an adaptive safe screening method for elastic net problem that efficiently utilizes previous solution as reference for enhanced power. In section 2 we will derive the safe screening rule and build it into the adaptive screening framework to form an adaptive screening method. In section 3, we will run experiments on both synthetic and real data sets to compare our method with the other state-of-the-art screening method strong rule screening. In section 4, we will discuss the results and some remarks.



\section{Screening Method}
\subsection{Problem Formulation}

A dual form of problem \eqref{eq:enet} introducing extra variables can be given by (please see appendix for details):

\begin{gather}
        \label{eq:dualtheta}
        \underset{\boldsymbol\theta\in \mathbb{R}^{ n},\boldsymbol\gamma\in\mathbb{R}^p}{\mathrm{max}}g_\lambda(\boldsymbol\theta,\boldsymbol\gamma)\equiv\frac{1}{2}||\boldsymbol y||_2^2-\frac{\lambda^2}{2}\left\Vert\boldsymbol\theta-\frac{\boldsymbol y}{\lambda}\right\Vert_2^2-\frac{\lambda^2}{2}||\boldsymbol\gamma||_2^2\\
        \begin{aligned}s.t.\quad (\boldsymbol\theta,\boldsymbol\gamma)\in \mathcal{F}_\lambda\equiv\{(\boldsymbol\theta,\boldsymbol\gamma):\quad
            &||X^T\boldsymbol\theta-\sqrt{n(1-\alpha)\lambda}\boldsymbol\gamma||_\infty\leq n\alpha\}\nonumber,
        \end{aligned}
\end{gather}

where $\boldsymbol\theta\in \mathbb{R}^{n}$ and $\boldsymbol\gamma\in\mathbb{R}^p$ are the dual variables. We will use $\boldsymbol\mu\in \mathbb{R}^{n+p}$ to denote the combined vector $(\boldsymbol \theta,\boldsymbol\gamma)$. The dual problem becomes the minimization of the convex function $g(\boldsymbol\mu)$ within a convex feasible set $\mathcal{F}_\lambda$. Let $\boldsymbol\beta_\lambda$ denote the solution to the primal problem at penalty parameter value $\lambda$ and $\boldsymbol\mu_{\lambda}=(\boldsymbol\theta_{\lambda},\boldsymbol\gamma_\lambda)$ denote the corresponding dual solution. The primal solution and dual solution can be connected by

\begin{equation}
    \label{eq:dualprimal}
    \boldsymbol\theta_\lambda=\frac{\boldsymbol y-X\boldsymbol\beta_\lambda}{\lambda},\quad \boldsymbol\gamma_\lambda=\sqrt{\frac{n(1-\alpha)}{\lambda}}\boldsymbol\beta_\lambda,
\end{equation}

and the KKT conditions for the primal problem~\eqref{eq:enet} can be expressed as:

\begin{gather}
    \label{eq:kktenet}
    \begin{aligned}&\boldsymbol\beta_{\lambda,j}=0\implies|\boldsymbol x_j^T\boldsymbol\theta_\lambda|\leq n\alpha\\
    & \boldsymbol\beta_{\lambda,j}\neq0\implies  \boldsymbol x_j^T\boldsymbol\theta_\lambda-n(1-\alpha)\boldsymbol\beta_{\lambda,j}=n\alpha\textit{sign}(\boldsymbol\beta_{\lambda,j}).
    \end{aligned}
\end{gather}

for any $j$. Combining \eqref{eq:dualprimal} and \eqref{eq:kktenet} we have a trivial closed form solution for the problem at some $\lambda$ values:

\begin{gather}
    \label{eq:lammax}
    \begin{aligned}
        \boldsymbol\beta_\lambda=0\iff \lambda \geq \lambda_{\max}\equiv \max_j \frac{|\boldsymbol x_j^T\boldsymbol y|}{n\alpha}
    \end{aligned}
\end{gather}

Thus, when solving the problems on a grid of $L+1$ decreasing $\lambda$ values: $\lambda_0>\lambda_1>...>\lambda_L>0$, it makes more sense to choose $\lambda_0\leq\lambda_{\max}$ to take advantage of the known solution. If an algorithm solve the problems sequentially in decreasing order of $\lambda$, then solution at $\lambda_{l'}$ will be known before solving the problem at $\lambda_l$ if $l'<l$. In the rest of the paper, we will derive screening method for such a path-wise structure. Also, without loss of generality, we will derive the screening method for the problem at $\lambda_1$ assuming the solution at $\lambda_0$ is known and can be used as a reference for screening. The same method can be applied to any pair of $\lambda_{l}$ and $\lambda_{l'}$. For simplicity, we will use $\boldsymbol\beta_l,\boldsymbol\mu_l,\boldsymbol\theta_l,\boldsymbol\gamma_l$ to denote the solution at any $\lambda_l$.

The KKT conditions \eqref{eq:kktenet} also say that for any $\lambda_1$ if 

\begin{equation}
    \label{eq:disc_cond}
    |\boldsymbol x_j^T\boldsymbol\theta_{1}|<n\alpha,
\end{equation}

we can safely conclude $\boldsymbol\beta_{1,j}=0$ and the corresponding $\boldsymbol x_j$ can be discarded for the optimization at $\lambda_1$. Although the left hand side of \eqref{eq:disc_cond} is unknown until the solution is obtained at the target $\lambda_1$, we can use the solution at the reference $\lambda_{0}$ to derive some upper bound for the left hand side: $T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)\geq |\boldsymbol x_j^T\boldsymbol\theta_1|$ and then if $T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)<n\alpha$ we can also safely conclude $\boldsymbol\beta_{1,j}=0$. The goal is to find a smaller $T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)$ to make more discards. Note as $\lambda$ changes, both the dual function $g_\lambda$ and the feasible set $\mathcal{F}_\lambda$ change. To simplify the construction of the upper bound, we consider the intermediate dual problem:

\begin{gather}
        \label{eq:dualmi}
        \boldsymbol\mu_{1|0}=(\boldsymbol\theta_{1|0},\boldsymbol\gamma_{1|0})\equiv\underset{\boldsymbol\mu\in \mathbb{R}^{ n+p}}{\mathrm{arg\,max}}\,g_{\lambda_0}(\boldsymbol\mu)\\
        \begin{aligned}s.t.\quad \boldsymbol\mu\in \mathcal{F}_{\lambda_1}\nonumber.
        \end{aligned}
\end{gather}

On one hand, compared to the origin dual problem \eqref{eq:dualtheta} at the reference $\lambda_0$, the intermediate problem \eqref{eq:dualmi} optimizes the same dual function on a slightly reshaped feasible set. On the other hand, compared to the intermediate problem, the origin dual problem at the target $\lambda_1$ optimize a slightly different dual function on the same feasible set. Each pair of problems become more similar. Then we can find a region $\mathcal{A}^1(\lambda_1,\lambda_0|\boldsymbol\mu_0)$ based on the reference solution $\boldsymbol\mu_0$, that contains the intermediate solution ($\boldsymbol\mu_{1|0}\in \mathcal{A}^1(\lambda_1,\lambda_0|\boldsymbol\mu_0)$) and after that find a region $\mathcal{A}^2(\lambda_1,\lambda_0|\boldsymbol\mu_{1|0})$ based on the intermediate solution $\boldsymbol\mu_{1|0}$, that contains the target solution  ($\boldsymbol\mu_1\in \mathcal{A}^2(\lambda_1,\lambda_0|\boldsymbol\mu_{1|0})$). Last if we find an upper bound for the double maximization problem in the two regions:

\begin{equation}
    \label{eq:boundbound}
    T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)\geq \underset{\boldsymbol\mu'\in\mathcal{A}^1(\lambda_1,\lambda_0|\boldsymbol\mu_0)}{\mathrm{max}}\,\underset{\boldsymbol\mu\in\mathcal{A}^2(\lambda_1,\lambda_0|\boldsymbol\mu')}{\mathrm{max}}|\boldsymbol x_j^T\boldsymbol\theta|,
\end{equation}

then it automatically satisfies $T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)\geq |\boldsymbol x_j^T\boldsymbol\theta_1|$ and can be safely used for screening. Note there is also a slightly different alternative method to construct an intermediate problem and the results will be briefly discussed in the appendix.

In the special case when $\lambda_0=\lambda_{\max}$ as defined in \eqref{eq:lammax}, a form of the bound $T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)$ has been derived in BEDPP for elastic net \citep{Zeng2021}:

\begin{theorem}
    \label{thm:0.1}
    For any $\lambda_1\in(0,\lambda_{0})$, let $\boldsymbol x_*\equiv\underset{\boldsymbol x_j}{argmax}|\boldsymbol x_j^T\boldsymbol y|$ and $c\equiv \frac{\lambda_{0}-\lambda_1}{\lambda_{0}\lambda_1}$, if $\lambda_0=\lambda_{\max}$, then
    \begin{gather}
        \begin{aligned}
            T_j(\lambda_{1},\lambda_{0}|\boldsymbol\mu_0)&=\left|\frac{\lambda_{0}+\lambda_1}{2\lambda_{0}\lambda_1}\boldsymbol x_j^T\boldsymbol y-\frac{n\alpha c\lambda_{0}}{2(||\boldsymbol x_*||_2^2+n(1-\alpha)\lambda_1)}\textit{sign}(\boldsymbol x_j^T\boldsymbol y) \boldsymbol x_j^T\boldsymbol x_*\right|+\\
            &\frac{c}{2}\sqrt{\left(||\boldsymbol x_j||_2^2+n(1-\alpha)\lambda_1\right)\left(||\boldsymbol y||_2^2-\frac{n^2\alpha^2\lambda_{0}^2}{||\boldsymbol x_*||_2^2+n(1-\alpha)\lambda_1}\right)}.
        \end{aligned}
    \end{gather}
\end{theorem}

Thus, in rest of the derivation, we will focus on the the case when $\lambda_0<\lambda_{\max}$. These two cases together will cover the whole solution path.

\subsection{Intermediate and Target Dual Solutions Regions}

In this section, we will derive the two regions $\mathcal{A}^1$ and $\mathcal{A}^2$.

\begin{theorem}
    \label{thm:1.1}
    For any $\lambda_1<\lambda_{0}\in (0,\lambda_{max})$, assuming $\boldsymbol\mu_0$ is known, $\boldsymbol\mu_{1|0}$ is contained in the set $\mathcal{A}^1(\lambda_1,\lambda_0|\boldsymbol\mu_0)$ that is a ball with center and radius
    \begin{gather}
        \begin{aligned}
            \boldsymbol c_1&=\binom{\boldsymbol\theta_0}{\sqrt{\frac{\lambda_1}{\lambda_0}}\boldsymbol\gamma_0}\\
            r_1&=\sqrt{c(\lambda_0-\lambda_1)}||\boldsymbol\gamma_0||_2,
        \end{aligned}
    \end{gather}
\end{theorem}

where $c\equiv\frac{\lambda_0-\lambda_1}{\lambda_0\lambda_1}$. Looking at the form of the dual function \eqref{eq:dualtheta}, $\boldsymbol\mu_1=(\boldsymbol\theta_1,\boldsymbol\gamma_1)$ is the projection of $(\frac{\boldsymbol y}{\lambda_1},0)$ onto $\mathcal{F}_{\lambda_1}$, while in the intermediate problem \eqref{eq:dualmi}, $\boldsymbol\mu_{1|0}=(\boldsymbol\theta_{1|0},\boldsymbol\gamma_{1|0})$ is the projection of a different point $(\frac{\boldsymbol y}{\lambda_0},0)$ onto the same set $\mathcal{F}_{\lambda_1}$. Using properties of projection onto a convex set as in the enhanced dual polytope projection (EDPP) \citep{wang2013lasso}, a region that contains the target $\boldsymbol\mu_1$ can be derived:

\begin{theorem}
    \label{thm:1.2}
    For any $t\geq0$ and $\lambda_1<\lambda_{0}\in (0,\lambda_{max})$, assuming $\boldsymbol\mu_{1|0}$ is known, $\boldsymbol\mu_1$ is contained in the set $\mathcal{A}^2(\lambda_1,\lambda_0,t|\boldsymbol\mu_{1|0})$ that is a ball with center and radius
    \begin{gather}
        \begin{aligned}
            \boldsymbol c_2\equiv\binom{\boldsymbol c_2^\theta}{\boldsymbol c_2^\gamma}&=\binom{\frac{1}{2}(\frac{1-t}{\lambda_0}+c)\boldsymbol y+\frac{t+1}{2}\boldsymbol\theta_{1|0}}{\frac{t+1}{2}\boldsymbol\gamma_{1|0}},\\
            r_2&=\frac{1}{2\lambda_0}\left\Vert\binom{(1-t)(\boldsymbol y-\lambda_0\boldsymbol\theta_{1|0})+c\lambda_0\boldsymbol y}{(1-t)\lambda_0\boldsymbol\gamma_{1|0}}\right\Vert_2,
        \end{aligned}
    \end{gather}
\end{theorem}

Note the result is valid for any $t\geq 0$. Instead of choosing a $t$ at this step as in the EDPP method, we will decide the choice of $t$ in the end of next subsection.

\subsection{Upper Bound of the Double Maximization}


To find the bound in \eqref{eq:boundbound}, we can first consider the first maximization problem:

\begin{equation}
    \Tilde{T}_j(\lambda_1,\lambda_0|\boldsymbol\mu')\equiv\underset{\boldsymbol\mu\in\mathcal{A}^2(\lambda_1,\lambda_0|\boldsymbol\mu')}{\mathrm{max}}|\boldsymbol x_j^T\boldsymbol\theta|,
\end{equation}

and it can be broken into two sub-problems:

\begin{equation}
    \label{eq:ttilde}
    \Tilde{T}^\xi_j(\lambda_1,\lambda_0|\boldsymbol\mu')\equiv\underset{\boldsymbol\mu\in\mathcal{A}^2(\lambda_1,\lambda_0|\boldsymbol\mu')}{\mathrm{max}}\xi \boldsymbol x_j^T\boldsymbol\theta,
\end{equation}

where $\xi\in\{-1,1\}$. The sub-problem \eqref{eq:ttilde} is maximizing a linear function in a ball with center $\boldsymbol c_2$ and radius $r_2$ and the maximum is easy to obtain:

\begin{gather}
    \label{eq:ttildexi}
    \begin{aligned}
        \Tilde{T}^\xi_j(\lambda_1,\lambda_0,t|\boldsymbol\mu')=&\xi\boldsymbol x_j^T\boldsymbol c_2^\theta+||\boldsymbol x_j||_2r_2\\
        =&\xi\left( \frac{1}{2}(\frac{1-t}{\lambda_0}+c)\boldsymbol x_j^T\boldsymbol y+\frac{t+1}{2}\boldsymbol x_j^T\boldsymbol\theta'\right)\\
        &+\frac{||\boldsymbol x_j||_2|1-t|}{2}\left\Vert\binom{\boldsymbol\theta'-\left(\frac{1}{\lambda_0}+\frac{c}{1-t}\right)\boldsymbol y}{\boldsymbol\gamma'}\right\Vert_2,\\
    \end{aligned}
\end{gather}

where we define $0\cdot||\boldsymbol v_1+\frac{\boldsymbol v_2}{0}||_2\equiv ||\boldsymbol v_2||_2$ for any vector $\boldsymbol v_1,\boldsymbol v_2$. There is an extra parameter $t$, and this bound will be valid for all $t\geq 0$. The maximum of $\Tilde{T}^\xi_j$ on $\mathcal{A}^1$ does not have a simple close form, so we consider an upper bound for it instead.

\begin{theorem}
    \label{thm:2.1}
    For any $\lambda_1<\lambda_{0}\in (0,\lambda_{max})$, $j=1,2,...,p$ and $\xi=-1,1$, assuming $\boldsymbol\mu_0$ is known, if we define
    \begin{align}
        \begin{gathered}
            T^\xi_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)\equiv\frac{\frac{1-t}{\lambda_0}+c}{2}\xi\boldsymbol x_j^T \boldsymbol y+\frac{t+1}{2}\xi \boldsymbol x_j^T \boldsymbol \theta_{0}\\+\frac{t+1+|1-t|}{2}||\boldsymbol x_j||_2\sqrt{c(\lambda_0-\lambda_1)}||\boldsymbol\gamma_{0}||_2+\frac{||\boldsymbol x_j||_2}{2}\left\Vert\binom{(1-t)\boldsymbol\theta_{0}-\left(\frac{1-t}{\lambda_0}+c\right)\boldsymbol y}{(1-t)\sqrt{\frac{\lambda_1}{\lambda_0}}\boldsymbol\gamma_{0}}\right\Vert_2\\
            %&=\xi \boldsymbol x_j^T \boldsymbol c_1^\theta+||\boldsymbol x_j||_2\left(\sqrt{c(\lambda_0-\lambda_1)}||\boldsymbol c_1^\gamma||_2+\sqrt{1+c(\lambda_0-\lambda_1)}r_1\right).
        \end{gathered}
    \end{align}
    then $T^\xi_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)\geq\underset{\boldsymbol\mu'\in\mathcal{A}^1(\lambda_1,\lambda_0|\boldsymbol\mu_0)}{\mathrm{max}}\Tilde{T}^\xi_j(\lambda_1,\lambda_0,t|\boldsymbol\mu')$ for all $t\geq0$.
\end{theorem}

If we define $\boldsymbol r_0\equiv \boldsymbol y-X\boldsymbol\beta_{0}$ and $\hat{\boldsymbol y}_{0}\equiv X\boldsymbol\beta_{0}$, then the results above can be expressed in primal variables:

\begin{gather}
    \begin{aligned}
        &T^\xi_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)=  \left(\frac{1-t}{2\lambda_0}+\frac{c}{2}\right)\xi\boldsymbol x_j^T \boldsymbol y+\frac{t+1}{2\lambda_0}\xi \boldsymbol x_j^T \boldsymbol r_{0}\\
        &+\frac{t+1+|1-t|}{2}||\boldsymbol x_j||_2\sqrt{n(1-\alpha) c^2\lambda_1}||\boldsymbol\beta_{0}||_2\\
        &+\frac{||\boldsymbol x_j||_2}{2\lambda_0}\sqrt{(1-t)^2||\hat{\boldsymbol y}_{0}||_2^2+c^2\lambda_0^2||\boldsymbol y||_2^2+2(1-t)c\lambda_0 \boldsymbol y^T\hat{\boldsymbol y}_{0}+(1-t)^2n(1-\alpha)\lambda_1||\boldsymbol\beta_{0}||_2^2}.
    \end{aligned}
\end{gather}

$T_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)$ being the maximum of it over $\xi=-1,1$ is

\begin{gather}
    \label{eq:t}
    \begin{aligned}
        &T_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)= \left| \left(\frac{1-t}{2\lambda_0}+\frac{c}{2}\right)\boldsymbol x_j^T \boldsymbol y+\frac{t+1}{2\lambda_0} \boldsymbol x_j^T \boldsymbol r_{0}\right|\\
        &+\frac{t+1+|1-t|}{2}||\boldsymbol x_j||_2\sqrt{n(1-\alpha) c^2\lambda_1}||\boldsymbol\beta_{0}||_2\\
        &+\frac{||\boldsymbol x_j||_2}{2\lambda_0}\sqrt{(1-t)^2||\hat{\boldsymbol y}_{0}||_2^2+c^2\lambda_0^2||\boldsymbol y||_2^2+2(1-t)c\lambda_0 \boldsymbol y^T\hat{\boldsymbol y}_{0}+(1-t)^2n(1-\alpha)\lambda_1||\boldsymbol\beta_{0}||_2^2}.
    \end{aligned}
\end{gather}

Last, we need to choose a $t\geq 0$. An optimal choice will be the $t$ that minimizes $T^\xi_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)$. This choice results in a complicated form of $t$ and different values of $t$ have to be chosen for different $\boldsymbol x_j$, which may result in additional computation cost that outweighs the gain from screening. Instead, we choose the $t$ that minimizes a simpler objective

\begin{equation}
    \frac{t}{2}||\tilde{\boldsymbol x}_j||_2\sqrt{c(\lambda_0-\lambda_1)}||\boldsymbol\gamma_{0}||_2+\frac{||\boldsymbol x_j||_2}{2}\left\Vert\binom{(1-t)\boldsymbol\theta_{0}-\left(\frac{1-t}{\lambda_0}+c\right)\boldsymbol y}{(1-t)\sqrt{\frac{\lambda_1}{\lambda_0}}\boldsymbol\gamma_{0}}\right\Vert_2.
\end{equation}

The solution will be $t=0$ if

\begin{equation}
    \label{eq:t0}
    \frac{1}{\lambda_0^2}||\hat{\boldsymbol y}_{0}||_2^2+\frac{2\lambda_1-\lambda_0}{\lambda_0\lambda_1}n(1-\alpha)||\boldsymbol\beta_{0}||_2^2\leq 0,
\end{equation}

else $t=$

\begin{align}
    \label{eq:tn0}
    \begin{gathered}
        \left(1+\frac{c\lambda_0\boldsymbol y^T\hat{\boldsymbol y}_{0}}{||\hat{\boldsymbol y}_{0}||_2^2+\lambda_1n(1-\alpha)||\boldsymbol\beta_{0}||_2^2}\right.\\
        \left.-c^2\lambda_0^2\sqrt{\frac{||\boldsymbol y||_2^2\left(||\hat{\boldsymbol y}_{0}||_2^2+\lambda_1n(1-\alpha)||\boldsymbol\beta_{0}||_2^2\right)-\boldsymbol y^T\hat{\boldsymbol y}_{0}}{||\hat{\boldsymbol y}_{0}||_2^2+\frac{\lambda_0(2\lambda_1-\lambda_0)}{\lambda_1}n(1-\alpha)||\boldsymbol\beta_{0}||_2^2}}
        \frac{||\boldsymbol\beta_{0}||_2\sqrt{\lambda_1n(1-\alpha)}}{||\hat{\boldsymbol y}_{0}||_2^2+\lambda_1n(1-\alpha)||\boldsymbol\beta_{0}||_2^2}\right)\vee0
    \end{gathered}
\end{align}

This choice of $t$ does not depend on $\boldsymbol x_j$ but will still be close to the optimal choice. The last two terms in \eqref{eq:t} will be the same across all $j$'s except for a multiplier of $||\boldsymbol x_j||_2$. We will denote $T_j(\lambda_1,\lambda_0,t|\boldsymbol\mu_0)$ with the $t$ defined above as $T_j(\lambda_1,\lambda_0|\boldsymbol\mu_0)$ and it will be the final upper bound we want.

\subsection{Safe Rule and Adaptive Safe Screening}

All the previous derivations can be summarized in the following safe rule that discard features in the problem given a previous solution:

\begin{theorem}
    \label{thm:rule}
    For any $\lambda_1<\lambda_{0}\in (0,\lambda_{max}]$, $j=1,2,...,p$ and assuming $\boldsymbol\mu_0$ is known,
    \begin{enumerate}
        \item If $\lambda_0=\lambda_{\max}$: $T_j(\lambda_1,\lambda_0|\boldsymbol\mu_0)$ is given Theorem \ref{thm:0.1}. Else $T_j(\lambda_1,\lambda_0|\boldsymbol\mu_0)$ is given in \eqref{eq:t},\eqref{eq:t0},\eqref{eq:tn0}.
        \item If $T_j(\lambda_1,\lambda_0|\boldsymbol\mu_0)<n\alpha$, then $[\boldsymbol\beta_{1}]_j=0$.
    \end{enumerate}
\end{theorem}

Note no matter whether $\lambda_0=\lambda_{\max}$ or not, the upper bound $T_j(\lambda_1,\lambda_0|\boldsymbol\mu_0)$ will becomes the upper bound in EDPP rule when $\alpha$ goes to 1. Our proposed rule can be considered as a generalization of EDPP rule with no impact on the performance in the case when $\alpha=1$.

An algorithm with adaptive safe screening framework \citep{wang2021adaptive} using the proposed safe screening rule can be built to solve the elastic net problem along a path of $L$ tuning parameter values $\lambda_1,...,\lambda_L$ as described below:

\begin{algorithm}[H]
  \label{alg:pathenet}
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Pre}{Pre-compute}\SetKwInOut{Initialize}{Initialize}
    \SetAlgoLined
    \Input{$\{\boldsymbol x_j\}_{j=1}^p, \boldsymbol y,\alpha,  \lambda_{1},...,\lambda_{L}$}
    \BlankLine
    \Initialize{$\{\boldsymbol x_j^T\boldsymbol y\}_{j=1}^p$, $\lambda_{ref}=\lambda_0=\lambda_{max}$}
    
    \For{$l\xleftarrow{}$ 1 \KwTo L}{
        Safe Screening: $\mathcal{S}_{l}\xleftarrow{}\{j:T_j(\lambda_l,\lambda_{ref}|\boldsymbol\mu_{ref})\geq n\alpha\}$ (features not discarded by Theorem \ref{thm:rule}).
        
        Strong Screening: $\mathcal{R}_{l}\xleftarrow{}\{j\in \mathcal{S}_{l}:x_j$ not discarded by any addition non-safe screening$\}$.
        
        \Initialize{$\boldsymbol \beta_l=\boldsymbol \beta_{l-1},\boldsymbol  r_{l}=\boldsymbol r_{l-1}$}
        
        \Repeat{Post-convergence check passes}{
            Solve elastic net problem using only features in $\mathcal{R}_{l}$, with warm start $\boldsymbol \beta_{l}, \boldsymbol r_{l}$ and any elastic net solving algorithm. 
            
            Save new $\boldsymbol \beta_{l}, \boldsymbol r_{l}$.
            
            Post-convergence check for all $j\in \mathcal{S}_{l}\setminus\mathcal{R}_{l}$,
            
            \uIf{Post-convergence check does not pass}{
                Add violating features to $\mathcal{R}_{l}$\;
            }
        }
        \uIf{Update conditions met}{
            $\lambda_{ref}=\lambda_{l}$
            
            Compute and save: $\{\boldsymbol x_j^T\boldsymbol r_{ref}\}_{j=1}^p, ||\boldsymbol\beta_{ref}||_2$.}
            
    }
    \Output{$\{\boldsymbol\beta_l\}_{l=1}^L$}
    \caption{Pathwise elastic algorithm with adaptive safe screening}
\end{algorithm}

Assuming the typical setting of a high-dimensional data set where $p\xrightarrow[]{}\infty$, the computational cost in safe screening (Theorem \ref{thm:0.1} or \eqref{eq:t},\eqref{eq:t0},\eqref{eq:tn0}) is approximately the cost of computing $\boldsymbol x_j^T\boldsymbol y,\boldsymbol x_j^T\boldsymbol x_*,\boldsymbol x_j^T\boldsymbol r_{ref}$ and $||\boldsymbol\beta_{ref}||_2$. The first two terms only need to be computed once and can be reused for the whole path. The latter two only need to be updated when updating the reference and can be reused until next update of reference. The rest steps of the safe screening can be considered as almost free.

In the step of strong screening, any addition screening methods can be implemented regardless of their safeness. For example, it can be active set cycling \citep{lee2007efficient}, strong rule \citep{Tibshirani2012}, no addition screening (simply $\mathcal{R}_l=\mathcal{S}_l$) or combination of screening methods. We will use a combination of strong rule on top of active set cycling in this steps as experiments show this can provide most speed up. Active set cycling is a simple idea that discards all the features that have zero coefficients in the previous solution $\lambda_{l-1}$ and does not require any computation. Strong rule discards feature $j$ if

\begin{equation}
    \label{eq:strong}
    \boldsymbol x_j^T\boldsymbol r_{ref}<\alpha(2\lambda_l-\lambda_{ref}).
\end{equation}

Since $\boldsymbol x_j^T\boldsymbol r_{ref}$ is already computed in the safe screening step, strong rule can also considered as free. If non-safe screening methods are used, after the convergence of the elastic net solving algorithm, a check on the KKT conditions \eqref{eq:kktenet} is required. Because KKT conditions are sufficient condition for the solution, if the check passes then there will be no incorrect discard. Else, we need to add the features that violate the KKT conditions into the problem and solve the problem. Because there is only a finite number of features, the check will pass after finite number if iteration. Moreover, safe screening step can narrow down the range of features that need to be checked from $1,..,p$ to $\mathcal{S}_l$ and thus make this step much faster.

Any elastic net solving algorithm can be used to solve the reduced problem in $\mathcal{R}_l$ as long as the algorithm converges to the minimum of the reduced problem. We choose pathwise coordinate descent \citep{friedman2007pathwise} because it has shown to outperform other algorithms especially in high-dimensional cases.

A naive choice of condition for updating safe screening reference will be always update and screening at any $\lambda_l$ is based on $\lambda_{l-1}$, which is the idea of sequential screening. Since we have shown almost all the cost of safe screening is the cost of updating, we may want to use the idea of adaptive screening \citep{wang2021adaptive} and reuse a reference $\lambda_{ref}$ multiple times until $\lambda_l$ such that

\begin{equation}
    (l-ref-1)|\mathcal{S}_{l}|-\sum_{l'=ref+1}^{l-1}|\mathcal{S}_{l'}|<p
\end{equation}

and update the reference to $\lambda_l$ at that point. This condition predicts the timing of updating that best optimizes the total cost of solving the whole path and has shown to outperform other state-of-the-art screening methods in lasso problem. We will show it provides significant speed up in elastic problem as well, compared to the naive sequential updating.

\section{Experiments}

We compared our proposed adaptive screening method (Adaptive), the sequential version of our proposed screening method (Sequential) and sequential strong rule for elastic net (SSR) in both simulated data sets and real data sets. SSR is the most efficient existing screening method for elastic net problem. Active cycling is incorporated into all methods. The elastic net solving algorithm is chosen to be the pathwise coordinate descent. All the methods are implemented and tested in the \textbf{biglasso} R package.

\subsection{Simulation Study}



The data is simulated from the model: $y=X\beta+0.1\epsilon$. The elements of $X$ and $\epsilon$ are sampled i.i.d from $N(0,1)$. The coefficient vector $\beta$ contains $s$ randomly selected elements equally spaced between $[-\beta_{\max},\beta_{\max}]$, with the remaining $p-s$ elements equal to zero. An elastic net problem is solved along a path of $L$ $\lambda$ values equally spaced on log scale between $(\lambda_{\max},\lambda_{\min}]$ where $\lambda_{\max}$ is determined by data. Each setting is replicated $10$ times to calculate the average time needed to solve the coefficient path. By default we set $p=10,000$, $n=1,000$, $s=20$, $\beta_{\max}=1$, $\alpha=0.9$, $L=100$, $\lambda_{\min}/\lambda_{\max}=0.05$. We consider the following cases where one of the variables above is varying at a time:

\begin{enumerate}
    \item \textbf{Varying proportion of lasso penalty:} $\alpha$ from $0.2$ to $0.999$.
    \item \textbf{Varying signal strength:} $\beta_{\max}$ from $0.05$ to $1$.
    \item \textbf{Varying sample size:} $n$ from $200$ to $10,000$.
    \item \textbf{Varying number of features:} $p$ from $1,000$ to $100,000$.
    \item \textbf{Varying sparsity:} $s$ from $10$ to $300$ while controlling total signal size $\beta_{\max}\times s=20$.
    \item \textbf{Varying number of grid points:}  $L$ from $20$ to $1000$.
    \item \textbf{Varying range of grid:}  $\lambda_{\min}/\lambda_{\max}$ from $0.5$ to $0.002$.
\end{enumerate}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{enet1.pdf}    \caption[Comparison of screening methods for elastic net in simulation 1]{Comparing speed of screening methods for elastic model under different settings. Top row: computation time in second. Bottom row: relative computation time compared to SSR. First column: Varying proportion of lasso penalty: $\alpha$. Second column: varying sample size $n$. Third column: varying number of features $p$. Fourth column: varying signal strength $\beta_{\max}$.}
    \label{fig:sim1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.82\textwidth]{enet2.pdf}    \caption[Comparison of screening methods for elastic net in simulation 2]{Comparing speed of screening methods for elastic net model under different settings. Top row: computation time in second. Bottom row: relative computation time compared to SSR. First column: varying sparsity $s$. Second column: Varying number of grid points $L$. Third column: Varying range of grid $\lambda_{\min}/\lambda_{\max}$.}
    \label{fig:sim2}
\end{figure}

The time spent on solving the whole path in seconds and the relative time compared to SSR method is summarized in Figure \ref{fig:sim1} and \ref{fig:sim2}. First, the sequential version of our proposed screening rule performs almost the same as SSR except in some extreme cases, and the adaptive version of our proposed screening rule outperforms the other two methods universally, with 3-4 $\times$ speedup in most cases. Second, the proposed adaptive method has the best performance when $\alpha$ is close to $1$. An $\alpha$ close to $1$ is a typical choice in practice because elastic problem becomes close to lasso problem and produces more sparse solution. Also, note $\alpha=1$, which corresponds to the lasso problem, is also included in the experiment, showing that our proposed method smoothly extends from lasso problem to a larger class of elastic net problem. Third, other factors show the adaptive screening is most favorable in a high-dimensional setting with large data size $n,p$ and small number of truly active features $s$ and in a path where $\lambda$ values are closely spaced when either the number of points on path $L$ is large or the path focuses on larger $\lambda$ values (larger $\lambda_{\min}/\lambda_{\max}$). Last, the effect of the signal strength $\boldsymbol\beta_{\max}$ has an interesting pattern. One would expect as the signal strengthens, screening methods should perform better at detecting 0 coefficients, which is true in the first halve of the range of $\boldsymbol\beta_{\max}$. After that, however, $\boldsymbol\beta_{\max}$ seems to have a negative effect on the adaptive method. The reason may be that we introduce an extra variable $\boldsymbol\gamma$ and result in a screening rule that is loose when $\boldsymbol\beta$ is large. Nevertheless, the adaptive method is still much faster than other methods regardless of the signal strength. We have also checked other factors, such as auto-correlation or block-correlation structure of the feature $X$ and number of threads in parallel computing, but we won't show the results here as there is no interesting pattern.

\subsection{Real Data}

\subsubsection{Traditional Data Sets}

In the section we compare the 3 screening methods on 4 real data sets that have traditional high-dimensional structures. An elastic net problem is solved along a grid of $L=100$ $\lambda$ values equally spaced on log scale between $(\lambda_{\max},\lambda_{\min}]$ where $\lambda_{\max}$ is determined by data and $\lambda_{\min}/\lambda_{\max}=0.05$. The 4 data sets are:

\begin{enumerate}
    \item \textbf{Breast cancer gene expression data
(GENE, \url{https://myweb.uiowa.edu/pbreheny/data/bcTCGA.html}):} this data has expression measurements of $p=17,322$ genes from $n=536$ patients. The response is the expression measurement of the gene BRCA1, which has been identified to be related to risk of breast cancer.
    \item \textbf{Cardiac fibrosis genome-wide association data
(GWAS, \url{https://arxiv.org/abs/1607.05636}):} this data set has $p=660,496$ single nucleotide
polymorphisms (SNPs) collected from $n=313$ human hearts. The response is the log of the ratio of cardiomyocytes to fibroblasts in the heart tissue, which is associated with heart failure.
    \item \textbf{MNIST handwritten image data
(MNIST, \url{http://yann.lecun.com/exdb/mnist}):} this data set has $60,000$ images in the training set and $10,000$ images in the testing set. Each image is a $28\times 28=784$ pixels image of handwritten digits. We use the training set to form a $n=784\times p=60,000$ feature matrix, treating pixels as samples and images as features. Then an image from the testing set is randomly selected as the response.
    \item \textbf{Subset of New York Times bag-of-words data
(NYT, \url{https://archive.ics.uci.edu/ml/datasets/Bag+of+Words}):} the raw data set is a $300,000\times 102,660$ matrix, each row being a document and each column being number of occurrence of a specific word in those documents. We selected $n=5,500$ documents by removing documents with low word counts. Then $55,000$ words are selected as the features and a randomly selected word is the response.
\end{enumerate}

For the GENE and GWAS data sets, the experiment is replicated 10 times, and the for the MNIST and NYT data sets, because the response is randomly selected, the experiment is replicated 50 times. The Figure \ref{fig:real} summarizes the comparison of time spent on solving the whole path with different screening methods and $\alpha$ values on the 4 data sets. The proposed adaptive method is the fastest especially when $\alpha$ is close to 1. Even if $\alpha$ is small, the adaptive is still the fastest most of the times.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.82\textwidth]{enetreal.pdf}    \caption[Comparison of screening methods for elastic net in 4 real data sets]{Average computing time in seconds with different screening methods and $\alpha$ for elastic net model on 4 data sets.}
    \label{fig:real}
\end{figure}

\iffalse
\begin{table}[ht]
\centering
\begin{tabular}{llll}
\toprule
Screening method & SSR & Sequential & Adaptive  \\
\midrule
$\alpha=0.95$ & 945 (4) & 949 (3) & \textbf{455 (2)} \\
$\alpha=0.9$ & 946 (3) & 949 (4) & \textbf{465 (2)}  \\
$\alpha=0.8$ & 939 (2) & 948 (2) & \textbf{471 (2)}  \\
$\alpha=0.6$ & 936 (2) & 949 (2) & \textbf{485 (2)}  \\
$\alpha=0.4$ & 938 (1) & 949 (1) & \textbf{500 (1)}  \\
$\alpha=0.2$ & 953 (4) & 966 (2) & \textbf{544 (3)} \\
\bottomrule
\end{tabular}
\caption{Average computing time in millisecond (standard error) for the GENE data set}
\label{Tab:gene}
\end{table}
\fi

\subsubsection{Ultra High-dimensional Data Sets}

In this section, we consider a ultra high-dimensional real data set that cannot be fit into the memory. This scenario is the greatest challenge to an optimizing algorithm and a good screening method will be most wanted. The data from the Genetic Risk Assessment of Defibrillator Events study (GRADE, \url{https://www.clinicaltrials.gov/ct2/show/NCT02045043}) is a genome-wide genetic data and were collected for 1,080 subjects diagnosed with cardiomyopathy and who had undergone surgery to receive an implantable cardioverter defibrillator in the past 5 years. After imputation using the Haplotype Reference Consortium as a reference genome and filtering for quality control \citep{Das2016}, there were $p=11,830,470$ SNPs which used as features. The size of the file storing the feature matrix is 96G and we conduct the test on a machine with 32G RAM. A variety of clinical data was collected on these individuals. To examine the performance of screening methods for elastic net problem, we focused on the numeric response left ventricular ejection fraction (LVEF). LVEF measures the proportion of blood being pumped out of the left ventricle of the heart with each contraction. Lower values are associated with more severe heart disease. LVEF data was available for $n=973$ subjects and they will be considered as the sample for this analysis. We chose a relatively large $\lambda_{\min} =0.1\lambda_{\max}$ to avoid having much more features selected than the number of observations. This choice will lead to a solution path with $1106$ to $4791$ features selected at the end of the path, depending on the value of $\alpha$. We choose number of $\lambda$ values to be the default value $L=100$ and test a grid of $\alpha$ values $\{1,0.95,0.9,0.8,0.6,0.4,0.2\}$. The time spent to solve the whole path in minutes for different screening methods and $\alpha$'s is shown in Figure~\ref{fig:lvef}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.72\textwidth]{enetlvef.pdf}    \caption[Comparison of screening methods for elastic net in LVEF analysis]{Average computing time in minutes with different screening methods and $\alpha$ for elastic net model in LVEF analysis on GRADE dataset.}
    \label{fig:lvef}
\end{figure}

\iffalse
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
\toprule
Screening method & SSR\,\,\,\,\,\,\,\,\,    & Sequential   & Adaptive  \\
\midrule
$\alpha=1$ & 87.6 & 89.3 & \textbf{52.3}  \\
$\alpha=0.95$ & 87.5 & 89.6 & \textbf{61.5}  \\
$\alpha=0.9$ & 88.1 & 89.5 & \textbf{66.8}  \\
$\alpha=0.8$ & 87.6 & 89.7 & \textbf{72.2}  \\
$\alpha=0.6$ & 88.2 & 90.7 & \textbf{77.8} \\
$\alpha=0.4$ & 88.8 & 91.7 & \textbf{84.4}  \\
$\alpha=0.2$ & \textbf{89.3} & 94.5 & 91.1  \\

\bottomrule
\end{tabular}
\caption{Average computing time in minutes the LVEF analysis with elastic net.\label{Tab:lvef}}
\end{table}
\fi

The proposed adaptive screening is the fastest in most cases, especially in the case when $\alpha$ is large. Even if $\alpha$ is extremely small, it does not do much harm to the speed. The proposed sequential screening performs very similar to strong rule screening.

\section{Discussion}

In this paper, we derived a safe screening rule for pathwise elastic net problem that utilizes previous solution as a reference for enhanced screening power by considering an intermediate problem that connects the reference and the target. We show that it can be built into a adaptive screening method that performs 3-4 times faster than the other state-of-the-art method SSR in most cases. The adaptive method is implemented in the public available \textbf{biglasso} R package.

Our result covers the more popular used form of elastic net problem \eqref{eq:enet}. This form is not only more useful in practice, but is also mathematical interesting because it cannot be reduced to lasso problem. In fact our proposed safe screening method is the only existing method we know of that works on penalty function with varying shape as $\lambda$ varies. A constant shape penalty $\lambda p(\boldsymbol\beta)$, such as lasso penalty or group lasso penalty can be rewritten as $p(\lambda\boldsymbol\beta)$, but the elastic net penalty does not share this property. Our result suggests possibility on a larger family of penalty functions.

Besides the standard elastic net problem we have covered, there is also a family of elastic net penalized generalized linear regression problems that can benefit from screening, but as far as we know, the strong rule screening is the only available screening method for these problems. Since our derivation has already tackled the trickiest part which is the varying shape of elastic net penalty function, similar technique can also help to derive safe rule and thus adaptive safe screening for elastic net penalized generalized regression problems, such as elastic net penalized logistic regression.